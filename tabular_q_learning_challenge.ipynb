{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "tabular-q-learning_challenge.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CLF6DZ-0Rtj"
      },
      "source": [
        "# Tabular Q-learning Challenge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_C_fusS0Rtn"
      },
      "source": [
        "First we import our required packages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3P8baiu0Rto"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "import sys"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RMidQc40Rto"
      },
      "source": [
        "Now we create a class that can hold our agent data. Our Q learning algorithm will utilize this class.\n",
        "\n",
        "Using a class like this enables us to make multiple agent_data objects with different parameters. This is very convenient for comparing the results of different training parameters in a parameter study.\n",
        "\n",
        "As you can see most parameters are referenced already.\n",
        "\n",
        "Fill in the gaps in the code using the Q-learning pseudocode."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bA65Km010Rtq"
      },
      "source": [
        "class q_learning_agent:\n",
        "    def __init__(self, alpha=1, epsilon=0.5, gamma=0.8, env_name=\"Taxi-v3\"):\n",
        "        self.alpha = alpha\n",
        "        self.epsilon = epsilon\n",
        "        self.gamma = gamma\n",
        "        self.q_table = None           #Is initialized when training the agent using the initialize_q_table function\n",
        "        self.env_name = env_name\n",
        "        self.env = None\n",
        "        self.initialize_q_table()\n",
        "    \n",
        "    def initialize_q_table(self):\n",
        "        self.env = gym.make(self.env_name)\n",
        "        self.q_table = #CODING ASSIGNMENT, 1 LINE: Define the shape of the Q table based on the pseudocode, and the OpenAI gym documentation.\n",
        "        self.env.close()\n",
        "    \n",
        "    def epsilon_greedy(self, observation):\n",
        "        #CODING ASSIGNMENT, ~5 LINES: Implement epsilon greedy.\n",
        "    \n",
        "    def train(self, episodes=2000):\n",
        "        self.env = gym.make(self.env_name)\n",
        "        \n",
        "        for _ in range(episodes):\n",
        "            state = self.env.reset() #Initialize state\n",
        "            done = False\n",
        "\n",
        "            while not done:\n",
        "                #CODING ASSIGNMENT, ~7 LINES: Implement the Q-learning algorithm.\n",
        "\n",
        "                #Select action with epsilon greedy\n",
        "\n",
        "                next_state, reward, done, _ = self.env.step(action) #The OpenAI Gym step function to execute an action and get the next state and reward.\n",
        "\n",
        "                # Retrieve old value from the q-table.\n",
        "\n",
        "                # Calculate td-error.\n",
        "\n",
        "                # Update q-value for current state using td-error.\n",
        "\n",
        "                # S <- S'\n",
        "\n",
        "                if done:\n",
        "                    state = self.env.reset()\n",
        "                    break\n",
        "        \n",
        "        self.env.close()\n",
        "\n",
        "    def test(self, episodes=200, render=False):\n",
        "        correct, incorrect = 0, 0\n",
        "        \n",
        "        self.env = gym.make(self.env_name)\n",
        "        for _ in range(episodes):\n",
        "            observation = self.env.reset()\n",
        "            done = False\n",
        "            while not done:\n",
        "                if render:\n",
        "                    self.env.render()\n",
        "                \n",
        "                action = np.argmax(self.q_table[observation])\n",
        "                observation, reward, done, _ = self.env.step(action)\n",
        "\n",
        "                if done:\n",
        "\n",
        "                    if (reward > 0):\n",
        "                        correct = correct + 1\n",
        "                    else:\n",
        "                        incorrect = incorrect + 1\n",
        "                    \n",
        "                    observation = self.env.reset()\n",
        "\n",
        "                    break\n",
        "\n",
        "        print(f\"Correct: {correct}, incorrect: {incorrect}\")\n",
        "        self.env.close()\n",
        "        return correct, incorrect\n",
        "\n",
        "    def show_episode(self):\n",
        "        self.test(episodes=1, render=True)\n",
        "        \n",
        "    def train_test_sequence(self, train_episodes=1000, test_episodes=200, repetitions=5):\n",
        "        self.initialize_q_table()\n",
        "        results = []\n",
        "        \n",
        "        print(f\"Training and testing in {repetitions} repetitions of {train_episodes} train episodes and {test_episodes} test episodes each.\")\n",
        "        print(f\"Parameters: alpha: {self.alpha}, epsilon: {self.epsilon}, gamma: {self.gamma}\")\n",
        "        \n",
        "        for i in range(repetitions):\n",
        "            self.train(episodes=train_episodes)\n",
        "            correct, incorrect = self.test(episodes=test_episodes)\n",
        "            results.append([correct, incorrect])\n",
        "        \n",
        "        return results\n",
        "        \n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hhMFBrd4Az1"
      },
      "source": [
        "Now we are ready to test our Q-learning agent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Lm_sozl0Rtr"
      },
      "source": [
        "agent = q_learning_agent(env_name=\"Taxi-v3\") #We instantiate the agent with base parameters\n",
        "agent.train()\n",
        "_ = agent.test()\n",
        "_ = agent.show_episode()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thpRTWoL0Rts"
      },
      "source": [
        "### Parameter Study\n",
        "We are going to do a parameter study on our agent for the Taxi-v3 environment. What this means is that we take a list of possible values for each parameter, and train our agent using all of these values, to get an idea of which value works best for which parameter.\n",
        "\n",
        "- Create lists of possible values for both gamma and epsilon.\n",
        "- Create for-loops where you set your agent's parameters to those values, and train\n",
        "- Find the optimal combination of values for gamma and epsilon.\n",
        "\n",
        "Use the train_test_sequence method for convenience."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rogrBHg10Rtt"
      },
      "source": [
        ""
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-vvKEee0Rtu"
      },
      "source": [
        "Now that you have found your (hopefully) optimal values, experiment with how few training episodes and repetitions you need to get a 100% correct when resetting and retraining your agent 3 times. <br> (The total amount of training episodes is train_episodes * repetitions)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cTAYs8R0Rtu"
      },
      "source": [
        ""
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmeCh3B90Rtv"
      },
      "source": [
        "### FrozenLake-v0\n",
        "Repeat the parameter study for the FrozenLake-v0 environment.\n",
        "This environment is stochastic: the movement direction depends only partially on the action taken by the agent.\n",
        "Because the environment is stochastic, alpha = 1 is no longer optimal. This is why you will now include alpha in your parameter study."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PmeobRV0Rtv"
      },
      "source": [
        ""
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxYR7muX4mgf"
      },
      "source": [
        "Now that you have found your (hopefully) optimal values, experiment with how few training episodes and repetitions you need to get a 100% correct when resetting and retraining your agent 3 times.\r\n",
        "(The total amount of training episodes is train_episodes * repetitions)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNauureX0Rtw"
      },
      "source": [
        ""
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHJD_Hkn0Rtw"
      },
      "source": [
        "### Extra parameter study: FrozenLake8x8-v0\n",
        "This is just FrozenLake-v0 but with a larger level. Because the level is larger, more steps need to be taken to get to a reward. These kinds of environments are called \"sparse reward\" environments. Because the agent currently only learns when it finds a reward, sparse reward environments can pose quite the problem. You probably will not be able to solve this yet, with the agent you currently have."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTcIONXC0Rtw"
      },
      "source": [
        ""
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inwG6FwX4oCK"
      },
      "source": [
        "Now that you have found your (hopefully) optimal values, experiment with how few training episodes and repetitions you need to get a 100% correct when resetting and retraining your agent 3 times.\r\n",
        "(The total amount of training episodes is train_episodes * repetitions)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGvYpdzj4izx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}